---

## 3Ô∏è‚É£ Add `app.py`

Now in your repo, click **Add file ‚Üí Create new file**  
Name it:

`app.py`

Paste this code üëá (simple but looks very professional + ‚Äúmemory‚Äù + tagging):

```python
import streamlit as st

st.set_page_config(page_title="CareLens AI", page_icon="üí¨", layout="wide")

st.title("üí¨ CareLens AI ‚Äî Customer Support Agent with Memory")
st.write(
    "An AI-powered support assistant that remembers your conversation, "
    "tags queries, and can be extended to use your FAQ / policy documents."
)

# Initialize memory
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "meta_history" not in st.session_state:
    st.session_state.meta_history = []

st.sidebar.header("‚ÑπÔ∏è Conversation Info")
st.sidebar.write("CareLens AI is a conceptual demo of a support agent with memory.")
st.sidebar.write("Memory is stored only for the current session.")

def classify_query(message: str):
    text = message.lower()
    category = "General"
    priority = "Normal"

    if any(k in text for k in ["price", "plan", "buy", "upgrade"]):
        category = "Sales"
    elif any(k in text for k in ["error", "bug", "issue", "not working", "problem", "crash"]):
        category = "Support"
    if any(k in text for k in ["refund", "urgent", "asap", "immediately", "complaint"]):
        priority = "High"

    return category, priority

def generate_response(history, user_message: str) -> str:
    """
    Placeholder for LLM integration.
    In real use, plug Llama-3 / GPT / Claude here.
    """
    # Build simple context from last few messages
    last_messages = [h["user"] for h in history[-3:] if "user" in h]
    context = "\n".join(last_messages)

    # Very simple mock "AI" reply
    base_reply = (
        "Thanks for your message! I'm an AI support assistant. "
        "Right now, this is a demo response. "
        "In a production setup, this would be generated by an LLM "
        "like Llama-3, GPT-4o, or Claude using your support docs."
    )

    if "refund" in user_message.lower():
        base_reply += " It looks like you mentioned a refund. Our typical refund window is 7‚Äì14 days, depending on policy."
    elif "pricing" in user_message.lower() or "price" in user_message.lower():
        base_reply += " You asked about pricing. Please check your plan details or contact sales for exact numbers."

    return base_reply

# Chat UI
st.subheader("üó®Ô∏è Chat with CareLens AI")

user_input = st.text_input("Type your customer message here:")

if st.button("Send") and user_input:
    category, priority = classify_query(user_input)
    reply = generate_response(st.session_state.chat_history, user_input)

    # Save both user + AI messages to history
    st.session_state.chat_history.append({"role": "user", "user": user_input})
    st.session_state.chat_history.append({"role": "assistant", "assistant": reply})
    st.session_state.meta_history.append(
        {"message": user_input, "category": category, "priority": priority}
    )

# Display conversation
chat_container, meta_container = st.columns([3, 1])

with chat_container:
    st.markdown("### üí¨ Conversation")
    if not st.session_state.chat_history:
        st.info("Start the conversation by sending a message above.")
    else:
        for msg in st.session_state.chat_history:
            if msg["role"] == "user":
                st.markdown(f"**You:** {msg['user']}")
            else:
                st.markdown(f"**CareLens AI:** {msg['assistant']}")
            st.markdown("---")

with meta_container:
    st.markdown("### üè∑Ô∏è Tags")
    if not st.session_state.meta_history:
        st.caption("Message tags (category & priority) will appear here.")
    else:
        for meta in st.session_state.meta_history[::-1]:
            st.markdown(f"**Message:** {meta['message'][:40]}...")
            st.markdown(f"- Category: `{meta['category']}`")
            st.markdown(f"- Priority: `{meta['priority']}`")
            st.markdown("---")
